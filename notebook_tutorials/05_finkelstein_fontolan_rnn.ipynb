{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finkelstein Fontolan RNN Loading & Dynamics\n",
        "\n",
        "This tutorial demonstrates how to load the pretrained recurrent neural network (RNN) from `configs/finkelstein_fontolan.py`. And how to pick the training distribution for the Koopman eigenfunction... which defines the domain of interest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the package when running in Colab\n",
        "# !pip install torchdiffeq\n",
        "# !pip install compose\n",
        "# !pip install --no-deps git+https://github.com/KabirDabholkar/separatrix_locator.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import importlib\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import scipy.io as sio\n",
        "\n",
        "from separatrix_locator.utils.finkelstein_fontolan_RNN import init_network, extract_opposite_attractors_from_model\n",
        "from separatrix_locator.utils.finkelstein_fontolan_task import initialize_task\n",
        "from separatrix_locator.dynamics.rnn import get_autonomous_dynamics_from_model, discrete_to_continuous\n",
        "from separatrix_locator.utils.odeint_utils import run_odeint_to_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_params_dict(params_mat_path: Path) -> dict:\n",
        "    params_mat = sio.loadmat(str(params_mat_path))\n",
        "    params_struct = params_mat[\"params\"][0, 0]\n",
        "\n",
        "    N = int(params_struct[\"N\"][0, 0])\n",
        "    M_full = np.asarray(params_struct[\"M\"], dtype=np.float32)\n",
        "    h_full = np.asarray(params_struct[\"h\"].flatten(), dtype=np.float32)\n",
        "    ramp_train = np.asarray(params_struct[\"ramp_train\"], dtype=np.float32)\n",
        "\n",
        "    params = {\n",
        "        \"N\": N,\n",
        "        \"dt\": float(params_struct[\"dt\"][0, 0]),\n",
        "        \"tau\": float(params_struct[\"tau\"][0, 0]),\n",
        "        \"f0\": float(params_struct[\"f0\"][0, 0]),\n",
        "        \"beta0\": float(params_struct[\"beta0\"][0, 0]),\n",
        "        \"theta0\": float(params_struct[\"theta0\"][0, 0]),\n",
        "        \"M\": M_full,\n",
        "        \"h\": h_full,\n",
        "        \"eff_dt\": float(params_struct[\"eff_dt\"][0, 0]),\n",
        "        \"sigma_noise_cd\": 100.0 / N,\n",
        "        \"des_out_left\": np.asarray(params_struct[\"des_out_left\"], dtype=np.float32),\n",
        "        \"des_out_right\": np.asarray(params_struct[\"des_out_right\"], dtype=np.float32),\n",
        "        \"ramp_train\": ramp_train,\n",
        "    }\n",
        "    return params\n",
        "\n",
        "package = importlib.import_module(\"separatrix_locator\")\n",
        "PROJECT_ROOT = Path(package.__file__).resolve().parents[2]\n",
        "PARAMS_ROOT = PROJECT_ROOT / \"rnn_params\" / \"finkelstein_fontolan\"\n",
        "INPUT_DATA_DIR = PARAMS_ROOT / \"input_data\"\n",
        "\n",
        "params_dict = load_params_dict(INPUT_DATA_DIR / \"params_data_wramp.mat\")\n",
        "# Helper utilities assume CPU tensors; keep everything on CPU for compatibility.\n",
        "device = torch.device(\"cpu\")\n",
        "model = init_network(params_dict, device=device)\n",
        "dim = int(params_dict[\"N\"])\n",
        "\n",
        "print(f\"Loaded Finkelstein Fontolan RNN with hidden dimension {dim} on {device}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = initialize_task(str(INPUT_DATA_DIR) + \"/\", N_trials_cd=10)\n",
        "\n",
        "discrete_dynamics = get_autonomous_dynamics_from_model(\n",
        "    model,\n",
        "    device=device,\n",
        "    rnn_submodule_name=None,\n",
        "    kwargs={\"deterministic\": True, \"batch_first\": False},\n",
        "    output_id=1,\n",
        ")\n",
        "continuous_dynamics = discrete_to_continuous(discrete_dynamics, delta_t=1.0)\n",
        "\n",
        "speed_factor = 60.0 ### make it so that bistability is observed at O(1) time.\n",
        "\n",
        "def dynamics_function(x: torch.Tensor, external_input: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    if external_input is None:\n",
        "        external_input = torch.zeros(x.shape[-2] if x.dim() > 1 else 1, 3, device=x.device)\n",
        "    return continuous_dynamics(x, external_input) * speed_factor\n",
        "\n",
        "\n",
        "input_range = (0.9, 0.92)\n",
        "with torch.no_grad():\n",
        "    attractors = extract_opposite_attractors_from_model(model, dataset, input_range=input_range)\n",
        "    static_external_input = torch.tensor([0.0, 0.0, input_range[0]], dtype=torch.float32, device=device)\n",
        "    trajectory = run_odeint_to_final(\n",
        "        lambda state, u: dynamics_function(state, u),\n",
        "        torch.tensor(attractors, dtype=torch.float32, device=device),\n",
        "        T=30,\n",
        "        inputs=static_external_input,\n",
        "        steps=10,\n",
        "        return_last_only=False,\n",
        "        no_grad=True,\n",
        "    )\n",
        "    attractors = trajectory[-1]\n",
        "print(f\"Integrated {trajectory.shape[0]} time steps for {trajectory.shape[1]} trajectories.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from separatrix_locator.core.separatrix_point import find_separatrix_point_along_line\n",
        "from separatrix_locator.distributions.gaussian import MultivariateGaussian, MultivariateGaussianList\n",
        "\n",
        "with torch.no_grad():\n",
        "    point_on_separatrix = find_separatrix_point_along_line(\n",
        "        dynamics_function,\n",
        "        static_external_input,\n",
        "        (torch.as_tensor(attractors[0]), torch.as_tensor(attractors[1])),\n",
        "        num_points=10,\n",
        "        num_iterations=4,\n",
        "        final_time=10,\n",
        "    ).cpu()\n",
        "\n",
        "attractor_a = torch.as_tensor(attractors[0]).cpu()\n",
        "attractor_b = torch.as_tensor(attractors[1]).cpu()\n",
        "attractor_vector = attractor_b - attractor_a\n",
        "attractor_distance = torch.linalg.norm(attractor_vector).item()\n",
        "unit_vector = attractor_vector / (torch.linalg.norm(attractor_vector) + 1e-8)\n",
        "identity = torch.eye(dim)\n",
        "\n",
        "convergence_threshold = 0.05 * attractor_distance\n",
        "\n",
        "print(f\"Point on separatrix shape: {tuple(point_on_separatrix.shape)}\")\n",
        "print(f\"Attractor distance: {attractor_distance:.3f}\")\n",
        "print(f\"Convergence threshold: {convergence_threshold:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "rng = torch.Generator(device=\"cpu\").manual_seed(0)\n",
        "\n",
        "def integrate_samples(samples: torch.Tensor, return_full: bool = False) -> torch.Tensor:\n",
        "    result = run_odeint_to_final(\n",
        "        lambda x, u: dynamics_function(x, u),\n",
        "        samples.to(device),\n",
        "        T=duration,\n",
        "        inputs=static_external_input,\n",
        "        steps=50,\n",
        "        return_last_only=not return_full,\n",
        "        no_grad=True,\n",
        "    )\n",
        "    return result.cpu()\n",
        "\n",
        "\n",
        "def min_distance_over_time(states: torch.Tensor) -> torch.Tensor:\n",
        "    dist_a = torch.linalg.norm(states - attractor_a, dim=-1)\n",
        "    dist_b = torch.linalg.norm(states - attractor_b, dim=-1)\n",
        "    return torch.minimum(dist_a, dist_b)\n",
        "\n",
        "\n",
        "def plot_pca_trajectories(trajectories: torch.Tensor, title: str, max_trajs: int = 10) -> None:\n",
        "    time_steps, batch_size, _ = trajectories.shape\n",
        "    limit = min(batch_size, max_trajs)\n",
        "    traj_subset = trajectories[:, :limit].reshape(time_steps * limit, -1).numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced = pca.fit_transform(traj_subset)\n",
        "    reduced = reduced.reshape(time_steps, limit, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for i in range(limit):\n",
        "        plt.plot(reduced[:, i, 0], reduced[:, i, 1], marker=\"o\", markersize=3, alpha=0.7)\n",
        "        plt.scatter(reduced[0, i, 0], reduced[0, i, 1], c=\"g\", marker=\"*\", s=80)\n",
        "        plt.scatter(reduced[-1, i, 0], reduced[-1, i, 1], c=\"r\", marker=\"X\", s=80)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PC 1\")\n",
        "    plt.ylabel(\"PC 2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def classify_convergence(final_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    dist_a = torch.linalg.norm(final_states - attractor_a, dim=1)\n",
        "    dist_b = torch.linalg.norm(final_states - attractor_b, dim=1)\n",
        "    min_dist = torch.minimum(dist_a, dist_b)\n",
        "    converged_mask = min_dist < convergence_threshold\n",
        "    return converged_mask, min_dist\n",
        "\n",
        "\n",
        "def summarize_convergence(mask: torch.Tensor) -> Dict[str, int]:\n",
        "    total = int(mask.shape[0])\n",
        "    converged = int(mask.sum().item())\n",
        "    return {\n",
        "        \"converged\": converged,\n",
        "        \"spurious\": total - converged,\n",
        "        \"total\": total,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: isotropic Gaussians of increasing scale\n",
        "This is naive choice. Let's see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 50\n",
        "duration = 30\n",
        "isotropic_scales = torch.tensor([2.5e-2, 5e-2, 1e-1, 5e-1, 1.0], dtype=torch.float32)\n",
        "base_sigma = attractor_distance\n",
        "isotropic_results: List[Dict[str, object]] = []\n",
        "\n",
        "for scale in isotropic_scales:\n",
        "    covariance = (scale * base_sigma) ** 2 * identity\n",
        "    proposal = torch.distributions.MultivariateNormal(point_on_separatrix, covariance_matrix=covariance)\n",
        "    samples = proposal.sample((num_samples,))\n",
        "    trajectories = integrate_samples(samples, return_full=True)\n",
        "    final_states = trajectories[-1]\n",
        "    min_distances_time = min_distance_over_time(trajectories)\n",
        "    converged_mask, min_distances_final = classify_convergence(final_states)\n",
        "    summary = summarize_convergence(converged_mask)\n",
        "    summary.update({\n",
        "        \"scale\": float(scale),\n",
        "        \"min_distance_time\": min_distances_time,\n",
        "        \"min_distance_final\": min_distances_final,\n",
        "        \"trajectories\": trajectories,\n",
        "    })\n",
        "    isotropic_results.append(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = len(isotropic_results)\n",
        "time_axis = torch.linspace(0.0, duration, isotropic_results[0][\"min_distance_time\"].shape[0]).numpy()\n",
        "fig, axes = plt.subplots(rows, 2, figsize=(6, 1.8 * rows))\n",
        "if rows == 1:\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for row, result in enumerate(isotropic_results):\n",
        "    min_distances_time = result[\"min_distance_time\"].numpy()\n",
        "    trajectories = result[\"trajectories\"]\n",
        "\n",
        "    # Calculate convergence using only the final time point for each trajectory\n",
        "    final_min_distances = min_distances_time[-1, :]  # last time point for each trajectory\n",
        "    converged_mask = final_min_distances < convergence_threshold\n",
        "\n",
        "    # Assign colours: blue for converged, orange for not converged\n",
        "    line_colours = [\"C0\" if conv else \"C1\" for conv in converged_mask]\n",
        "\n",
        "    # Plot each trajectory's min_distances_time as a separate line, colored by convergence\n",
        "    for idx in range(min_distances_time.shape[1]):\n",
        "        axes[row, 0].plot(time_axis, min_distances_time[:, idx], alpha=0.6, color=line_colours[idx])\n",
        "    axes[row, 0].axhline(convergence_threshold, color=\"r\", linestyle=\"--\")\n",
        "    axes[row, 0].annotate(f\"σ={result['scale']:.2g}\", xy=(0.05, 0.93), xycoords='axes fraction',\n",
        "                          fontsize=10, ha='left', va='top', bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.7))\n",
        "    # Remove individual ylabel; we'll add one common label later\n",
        "    # axes[row, 0].set_ylabel(\"min{dist to A,dist to B}\")\n",
        "    # axes[row, 0].set_title(f\"σ={result['scale']:.2g} min distance\")\n",
        "    if row == rows - 1:\n",
        "        axes[row, 0].set_xlabel(\"time\")\n",
        "\n",
        "    time_steps, batch_size, _ = trajectories.shape\n",
        "    limit = min(batch_size, 10)\n",
        "    traj_subset = trajectories[:, :limit].reshape(time_steps * limit, -1).numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced = pca.fit_transform(traj_subset).reshape(time_steps, limit, 2)\n",
        "    # Colour PCA lines by same converged_mask\n",
        "    for idx in range(limit):\n",
        "        color = \"C0\" if converged_mask[idx] else \"C1\"\n",
        "        axes[row, 1].plot(reduced[:, idx, 0], reduced[:, idx, 1], marker=\"o\", markersize=3, alpha=0.6, color=color)\n",
        "        axes[row, 1].scatter(reduced[0, idx, 0], reduced[0, idx, 1], c=\"g\", marker=\"*\", s=70)\n",
        "        axes[row, 1].scatter(reduced[-1, idx, 0], reduced[-1, idx, 1], c=\"r\", marker=\"X\", s=70)\n",
        "    axes[row, 1].annotate(f\"σ={result['scale']:.2g}\", xy=(0.05, 0.93), xycoords='axes fraction',\n",
        "                          fontsize=10, ha='left', va='top', bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.7))\n",
        "\n",
        "\n",
        "\n",
        "axes[0, 0].set_title(\"min{dist to A, dist to B}\", fontsize=12)\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = [f\"{result['scale']:.2g}\" for result in isotropic_results]\n",
        "converged_counts = [result[\"converged\"] for result in isotropic_results]\n",
        "spurious_counts = [result[\"spurious\"] for result in isotropic_results]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x, converged_counts, label=\"converged to A or B\")\n",
        "plt.bar(x, spurious_counts, bottom=converged_counts, label=\"went elsewhere\")\n",
        "plt.xticks(x, labels)\n",
        "plt.xlabel(\"Isotropic scale (relative σ)\")\n",
        "plt.ylabel(\"Number of trajectories\")\n",
        "plt.title(\"identifying the domain of interest\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Isotropic experiment summary:\")\n",
        "for result in isotropic_results:\n",
        "    print(\n",
        "        f\"scale={result['scale']:.2g}: converged={result['converged']}, spurious={result['spurious']} (total={result['total']})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "States converge to other 'spurious' attractors beyond the two of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Anisotropic gaussian, oblong along attractors, narrower along other axes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's pick the distribution more carefully to include the two attractors but make it narrow along the other dimensions. We express this in the covariance:\n",
        "\n",
        " $$\\Sigma = \\sigma_{\\perp}^2 I + (\\sigma_{\\parallel}^2 - \\sigma_{\\perp}^2)(\\mathbf{u}\\mathbf{u}^\\top),$$\n",
        " \n",
        " where $\\mathbf{u}$ is the unit vector along $(A - B)$ and $\\sigma_{\\parallel}$, $\\sigma_{\\perp}$ are the widths of the distribution along $\\mathbf{u}$ and perpendicular to it respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scale_factors = torch.tensor([5e-2, 1e-1, 5e-1, 1.0], dtype=torch.float32)\n",
        "\n",
        "normalised_dist = ((attractor_a - attractor_b)**2).mean().sqrt()\n",
        "\n",
        "anisotropic_sigma_a_multiplier = 3.0\n",
        "anisotropic_sigma_b = 3.0\n",
        "normalised_dist = ((attractor_a - attractor_b)**2).mean().sqrt()\n",
        "\n",
        "sigma_parallel = anisotropic_sigma_a_multiplier * normalised_dist\n",
        "sigma_perp = anisotropic_sigma_b\n",
        "base_covariance = (sigma_perp ** 2) * identity\n",
        "base_covariance += ((sigma_parallel ** 2) - (sigma_perp ** 2)) * torch.outer(unit_vector, unit_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anisotropic_results: List[Dict[str, object]] = []\n",
        "scaled_gaussians: List[MultivariateGaussian] = []\n",
        "\n",
        "for scale in scale_factors:\n",
        "    covariance = base_covariance * scale\n",
        "    proposal = torch.distributions.MultivariateNormal(point_on_separatrix, covariance_matrix=covariance)\n",
        "    samples = proposal.sample((num_samples,))\n",
        "    trajectories = integrate_samples(samples, return_full=True)\n",
        "    final_states = trajectories[-1]\n",
        "    min_distances_time = min_distance_over_time(trajectories)\n",
        "    converged_mask, min_distances_final = classify_convergence(final_states)\n",
        "    summary = summarize_convergence(converged_mask)\n",
        "    summary.update({\n",
        "        \"scale\": float(scale),\n",
        "        \"min_distance_time\": min_distances_time,\n",
        "        \"min_distance_final\": min_distances_final,\n",
        "        \"trajectories\": trajectories,\n",
        "    })\n",
        "    anisotropic_results.append(summary)\n",
        "\n",
        "    stable_samples = final_states[converged_mask]\n",
        "    if stable_samples.shape[0] < 2:\n",
        "        stable_samples = final_states\n",
        "    mean = stable_samples.mean(dim=0)\n",
        "    centered = stable_samples - mean\n",
        "    covariance_est = (centered.T @ centered) / max(stable_samples.shape[0] - 1, 1)\n",
        "    covariance_est += 1e-6 * identity\n",
        "    scaled_gaussians.append(MultivariateGaussian(dim=dim, mean=mean, covariance_matrix=covariance_est))\n",
        "\n",
        "ic_distribution_fit = MultivariateGaussianList(\n",
        "    scaled_gaussians,\n",
        "    name=\"FF_FIM_forward_empirical\",\n",
        "    scales=scale_factors.tolist(),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = len(anisotropic_results)\n",
        "time_axis = torch.linspace(0.0, duration, anisotropic_results[0][\"min_distance_time\"].shape[0]).numpy()\n",
        "fig, axes = plt.subplots(rows, 2, figsize=(6, 1.8 * rows))\n",
        "if rows == 1:\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for row, result in enumerate(anisotropic_results):\n",
        "    min_distances_time = result[\"min_distance_time\"].numpy()\n",
        "    trajectories = result[\"trajectories\"]\n",
        "\n",
        "    # Calculate convergence using only the final time point for each trajectory\n",
        "    final_min_distances = min_distances_time[-1, :]  # last time point for each trajectory\n",
        "    converged_mask = final_min_distances < convergence_threshold\n",
        "\n",
        "    # Assign colours: blue for converged, orange for not converged\n",
        "    line_colours = [\"C0\" if conv else \"C1\" for conv in converged_mask]\n",
        "\n",
        "    # Plot each trajectory's min_distances_time as a separate line, colored by convergence\n",
        "    for idx in range(min_distances_time.shape[1]):\n",
        "        axes[row, 0].plot(time_axis, min_distances_time[:, idx], alpha=0.6, color=line_colours[idx])\n",
        "    axes[row, 0].axhline(convergence_threshold, color=\"r\", linestyle=\"--\")\n",
        "    axes[row, 0].annotate(f\"σ={result['scale']:.2g}\", xy=(0.05, 0.93), xycoords='axes fraction',\n",
        "                          fontsize=10, ha='left', va='top', bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.7))\n",
        "    if row == rows - 1:\n",
        "        axes[row, 0].set_xlabel(\"time\")\n",
        "\n",
        "    time_steps, batch_size, _ = trajectories.shape\n",
        "    limit = min(batch_size, 10)\n",
        "    traj_subset = trajectories[:, :limit].reshape(time_steps * limit, -1).numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced = pca.fit_transform(traj_subset).reshape(time_steps, limit, 2)\n",
        "    # Colour PCA lines by same converged_mask\n",
        "    for idx in range(limit):\n",
        "        color = \"C0\" if converged_mask[idx] else \"C1\"\n",
        "        axes[row, 1].plot(reduced[:, idx, 0], reduced[:, idx, 1], marker=\"o\", markersize=3, alpha=0.6, color=color)\n",
        "        axes[row, 1].scatter(reduced[0, idx, 0], reduced[0, idx, 1], c=\"g\", marker=\"*\", s=70)\n",
        "        axes[row, 1].scatter(reduced[-1, idx, 0], reduced[-1, idx, 1], c=\"r\", marker=\"X\", s=70)\n",
        "    axes[row, 1].annotate(f\"σ={result['scale']:.2g}\", xy=(0.05, 0.93), xycoords='axes fraction',\n",
        "                          fontsize=10, ha='left', va='top', bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.7))\n",
        "\n",
        "axes[0, 0].set_title(\"min{dist to A, dist to B}\", fontsize=12)\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = [f\"{result['scale']:.2g}\" for result in anisotropic_results]\n",
        "converged_counts = [result[\"converged\"] for result in anisotropic_results]\n",
        "spurious_counts = [result[\"spurious\"] for result in anisotropic_results]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x, converged_counts, label=\"converged to A or B\")\n",
        "plt.bar(x, spurious_counts, bottom=converged_counts, label=\"went elsewhere\")\n",
        "plt.xticks(x, labels)\n",
        "plt.xlabel(\"Anisotropic scale factor\")\n",
        "plt.ylabel(\"Number of trajectories\")\n",
        "plt.title(\"identifying the domain of interest (anisotropic design)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(ic_distribution_fit)\n",
        "print(\"Anisotropic experiment summary:\")\n",
        "for result in anisotropic_results:\n",
        "    print(\n",
        "        f\"scale={result['scale']:.2g}: converged={result['converged']}, spurious={result['spurious']} (total={result['total']})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We managed to find distributions that are _mostly_ in the two-basin domain of interest."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
