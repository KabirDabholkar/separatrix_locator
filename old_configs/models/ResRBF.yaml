_target_: utils.setattrs_kwargs
target:
  _target_: architectures.AdditiveModel
  _args_  :
#    - _target_  : torch.nn.Linear
#      in_features   : ${....input_size}
#      out_features  : ${....output_size}
    - _target_: architectures.DeepResNet
      input_dim: ${....input_size}
      hidden_dim: ${....hidden_size}
      output_dim: ${....output_size}
      num_blocks: ${....num_layers}
  scale_output_by_inv_sqrt_hidden: ${..scale_last_layer_by_inv_sqrt_hidden}
  hidden_size: ${..hidden_size}
#      nonlin    :
#        _target_  : torch.nn.Identity
#
#    - _target_: architectures.DeepResNetVariableWidth
#      input_dim: ${....input_size}
#      hidden_dims: ${....hidden_sizes}
#      output_dim: ${....output_size}

#      nonlin  :
#        _target_ : torch.nn.ReLU

#    - _target_: torch.nn.Sequential
#      _args_:
#        - _target_: architectures.DeepResNet
#          input_dim : ${......input_size}
#          hidden_dim: 200
#          output_dim: 1
#          num_blocks: 1
#        - _target_: architectures.DeepResNet
#          input_dim : 1
#          hidden_dim: ${......hidden_size}
#          output_dim: ${......output_size}
#          num_blocks: ${......num_layers}

    # - _target_: learn_koopman_eig.partialised_RBF_maker
    #   in_features_dim: ${....input_size}
    #   num_kernels: ${....num_kernels}
    #   out_features_dim: ${....output_size}
    #   normalization: false
    #   #  normalization : true
    #   reset_params:
    #     upper_bound_kernels: 2
    #     std_shapes: 3

input_size  : ${dynamics.dim}
#hidden_size : 3000 #2
#num_layers  : 5

#hidden_size : 200
#num_layers  : 20

#hidden_size : 40
#num_layers  : 5

#hidden_sizes : [2048,1024,1024,512,512,256,256,128,128,64,64,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,16,16,8,8,4,4,2,2]



#hidden_sizes : [2048,1024,1024,512,512,256,256,128,128,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,16,16,8,8,4,4,2,2]
#hidden_sizes : [2048,1024,1024,512,512,256,256,128,128,64,64,32,32,16,16,8,8,4,4,2,2]
#hidden_sizes : [2048,1024,1024,512,512,256,256,128,128,64,64,32,32,16,16,8,8,4,4,2,2]
#hidden_sizes : [2048,1024,512,256,128,64,32,16,8,4,2] #1200 #80 #0 #1000 #1000 #2000 #2
#num_layers  : 5 #20 #20 #25 #5 #40 #25
output_size : 1 #7
num_kernels : 10 #0 #0

#hidden_size : 50
#num_layers  : 70

#hidden_size : 200
#num_layers  : 50

#hidden_size : 400
#num_layers  : 50

#hidden_size : 300
#num_layers  : 70

### For 2bit flip flop 3D and for 1bit flip flops <400D
#hidden_size : 400 #400
#num_layers  : 20

### For 1bit flip flop 512D
hidden_size : 550
num_layers  : 20

#hidden_size : 400 #400
#num_layers  : 6
#hidden_size : 800 #400
#num_layers  : 10

### best for microbiome 11D
#hidden_size : 1000 #400
#num_layers  : 25

#hidden_size : 1000 #400
#num_layers  : 40
name: AdditiveRBFResNet_hidden${.hidden_size}_output${.output_size}_numKernels${.num_kernels}_numlayers${.num_layers}
#name: AdditiveRBFLinearResNet_hidden${.hidden_size}_output${.output_size}_numKernels${.num_kernels}_numlayers${.num_layers}

#hidden_sizes : [16000,4000,2000,1000,500,100,50,10]
#name: AdditiveRBFResNet_superwidelayer8_output${.output_size}_numKernels${.num_kernels} #_numlayers${.num_layers}


#hidden_sizes : [16000,4000,2000]
#name: AdditiveRBFResNet_superwidelayer3_output${.output_size}_numKernels${.num_kernels} #_numlayers${.num_layers}

#hidden_sizes : [8096,4096,2048,1024,1024,512,512,256,256,128,128]
#name: AdditiveRBFResNet_evenwidertaperinglayers11_output${.output_size}_numKernels${.num_kernels} #_numlayers${.num_layers}

#hidden_sizes : [4096,4096,2048,1024,1024,512,512,256,256,128,128]
#name: AdditiveRBFResNet_widetaperinglayers11_output${.output_size}_numKernels${.num_kernels} #_numlayers${.num_layers}


#name: AdditiveRBFResNet_taperinglayers51_output${.output_size}_numKernels${.num_kernels} #_numlayers${.num_layers}

# Optional scaling of last layer by 1/sqrt(hidden_size)
scale_last_layer_by_inv_sqrt_hidden: true
